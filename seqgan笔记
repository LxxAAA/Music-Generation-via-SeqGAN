1、背景
GAN作为生成模型的一种新型训练方法，通过discriminative model来指导generative model的训练，并在真实数据中取得了很好的效果。尽管如此，
当目标是一个待生成的非连续性序列时，该方法就会表现出其局限性。非连续性序列生成，比如说文本生成，为什么单纯的使用GAN没有取得很好的效果呢？
主要的屏障有两点：

1）在GAN中，Generator是通过随机抽样作为开始，然后根据模型的参数进行确定性的转化。通过generative model G的输出，
discriminative model D计算的损失值，根据得到的损失梯度去指导generative model G做轻微改变，从而使G产生更加真实的数据。
而在文本生成任务中，G通常使用的是LSTM，那么G传递给D的是一堆离散值序列，即每一个LSTM单元的输出经过softmax之后再取argmax
或者基于概率采样得到一个具体的单词，那么这使得梯度下架很难处理。

2）GAN只能评估出整个生成序列的score/loss，不能够细化到去评估当前生成token的好坏和对后面生成的影响。

强化学习可以很好的解决上述的两点。再回想一下Policy Gradient的基本思想，即通过reward作为反馈，增加得到reward大的动作出现的概率，
减小reward小的动作出现的概率，如果我们有了reward，就可以进行梯度训练，更新参数。如果使用Policy Gradient的算法，当G产生一个单词时，
如果我们能够得到一个反馈的Reward，就能通过这个reward来更新G的参数，而不再需要依赖于D的反向传播来更新参数，因此较好的解决了上面所说的第一个屏障。
对于第二个屏障，当产生一个单词时，我们可以使用蒙塔卡罗树搜索(Alpho Go也运用了此方法)立即评估当前单词的好坏,
而不需要等到整个序列结束再来评价这个单词的好坏。

因此，强化学习和对抗思想的结合，理论上可以解决非连续序列生成的问题，而SeqGAN模型，正是这两种思想碰撞而产生的可用于文本序列生成的模型。



如果当前的cell是最后的一个cell，即我们已经得到了一个完整的序列，那么此时很好办，直接把这个序列扔给Discriminator，
得到输出为1的概率就可以得到reward值。如果当前的cell不是最后一个cell，即当前的单词不是最后的单词，我们还没有得到一个完整的序列，
如何估计当前这个单词的reward呢？我们用到了蒙特卡罗树搜索的方法。即使用前面已经产生的序列，从当前位置的下一个位置开始采样，得到一堆完整的序列。
在原文中，采样策略被称为roll-out policy，这个策略也是通过一个神经网络实现，这个神经网络我们可以认为就是我们的Generator。得到采样的序列后，
我们把这一堆序列扔给Discriminator，得到一批输出为1的概率，这堆概率的平均值即我们的reward。这部分正如过程示意图中的下面一部分：




可以说，模型我们已经介绍完了，但是在实验部分，论文中引入了一个新的模型中，被称为oracle model。这里的oracle如何翻译，我还真的是不知道，
总不能翻译为甲骨文吧。这个oracle model被用来生成真实的序列，可以认为这个model就是一个被训练完美的lstm模型，输出的序列都是real-world数据。
论文中使用这个模型的原因有两点：首先是可以用来产生训练数据，另一点是可以用来评价我们Generator的真实表现

