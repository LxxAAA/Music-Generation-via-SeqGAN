1、背景
GAN作为生成模型的一种新型训练方法，通过discriminative model来指导generative model的训练，并在真实数据中取得了很好的效果。尽管如此，
当目标是一个待生成的非连续性序列时，该方法就会表现出其局限性。非连续性序列生成，比如说文本生成，为什么单纯的使用GAN没有取得很好的效果呢？
主要的屏障有两点：

1）在GAN中，Generator是通过随机抽样作为开始，然后根据模型的参数进行确定性的转化。通过generative model G的输出，
discriminative model D计算的损失值，根据得到的损失梯度去指导generative model G做轻微改变，从而使G产生更加真实的数据。
而在文本生成任务中，G通常使用的是LSTM，那么G传递给D的是一堆离散值序列，即每一个LSTM单元的输出经过softmax之后再取argmax
或者基于概率采样得到一个具体的单词，那么这使得梯度下架很难处理。

2）GAN只能评估出整个生成序列的score/loss，不能够细化到去评估当前生成token的好坏和对后面生成的影响。

强化学习可以很好的解决上述的两点。再回想一下Policy Gradient的基本思想，即通过reward作为反馈，增加得到reward大的动作出现的概率，
减小reward小的动作出现的概率，如果我们有了reward，就可以进行梯度训练，更新参数。如果使用Policy Gradient的算法，当G产生一个单词时，
如果我们能够得到一个反馈的Reward，就能通过这个reward来更新G的参数，而不再需要依赖于D的反向传播来更新参数，因此较好的解决了上面所说的第一个屏障。
对于第二个屏障，当产生一个单词时，我们可以使用蒙塔卡罗树搜索(Alpho Go也运用了此方法)立即评估当前单词的好坏,
而不需要等到整个序列结束再来评价这个单词的好坏。

因此，强化学习和对抗思想的结合，理论上可以解决非连续序列生成的问题，而SeqGAN模型，正是这两种思想碰撞而产生的可用于文本序列生成的模型。



如果当前的cell是最后的一个cell，即我们已经得到了一个完整的序列，那么此时很好办，直接把这个序列扔给Discriminator，
得到输出为1的概率就可以得到reward值。如果当前的cell不是最后一个cell，即当前的单词不是最后的单词，我们还没有得到一个完整的序列，
如何估计当前这个单词的reward呢？我们用到了蒙特卡罗树搜索的方法。即使用前面已经产生的序列，从当前位置的下一个位置开始采样，得到一堆完整的序列。
在原文中，采样策略被称为roll-out policy，这个策略也是通过一个神经网络实现，这个神经网络我们可以认为就是我们的Generator。得到采样的序列后，
我们把这一堆序列扔给Discriminator，得到一批输出为1的概率，这堆概率的平均值即我们的reward。这部分正如过程示意图中的下面一部分：




可以说，模型我们已经介绍完了，但是在实验部分，论文中引入了一个新的模型中，被称为oracle model。这里的oracle如何翻译，我还真的是不知道，
总不能翻译为甲骨文吧。这个oracle model被用来生成真实的序列，可以认为这个model就是一个被训练完美的lstm模型，输出的序列都是real-world数据。
论文中使用这个模型的原因有两点：首先是可以用来产生训练数据，另一点是可以用来评价我们Generator的真实表现
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++

1. 背景
GAN在之前发的文章里已经说过了，不了解的同学点我，虽然现在GAN的变种越来越多，用途广泛，但是它们的对抗思想都是没有变化的。
简单来说，就是在生成的过程中加入一个可以鉴别真实数据和生成数据的鉴别器，使生成器G和鉴别器D相互对抗，D的作用是努力地分辨真实数据和生成数据，
G的作用是努力改进自己从而生成可以迷惑D的数据。当D无法再分别出真假数据，则认为此时的G已经达到了一个很优的效果。 
它的诸多优点是它如今可以这么火爆的原因： 
- 可以生成更好的样本 
- 模型只用到了反向传播,而不需要马尔科夫链 
- 训练时不需要对隐变量做推断 
- G的参数更新不是直接来自数据样本,而是使用来自D的反向传播 
- 理论上,只要是可微分函数都可以用于构建D和G,因为能够与深度神经网络结合做深度生成式模型

它的最后一条优点也恰恰就是它的局限，之前我发过的文章中也有涉及到，点点点点点我，在NLP中，数据不像图片处理时是连续的，可以微分，
我们在优化生成器的过程中不能找到“中国 + 0.1”这样的东西代表什么，因此对于离散的数据，普通的GAN是无法work的。

因为不完整的轨迹产生的reward没有实际意义，因此在原有y_1到y_t-1的情况下，产生的y_t的Q值并不能在y_t产生后直接计算，
除非y_t就是整个序列的最后一个。paper中想了一个办法，使用蒙特卡洛搜索（就我所知“蒙特卡洛”这四个字可以等同于“随意”）将y_t后的内容进行补全。
既然是随意补全就说明会产生多种情况，paper中将同一个y_t后使用蒙特卡洛搜索补全的所有可能的sequence全都计算reward，然后求平均

++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
+++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++
D:SeqGAN是比较早做的，16年底放在arxiv上，但是效果目前可能有更好的，比如lecun刚放在arxiv上他们投17年nips的文章，用了wgan；IRGAN和SeqGAN都是UCL的Wang Jun老师和上交的Zhang Weinan组做的。
E:irgan有点像以前八十年代的minmax game theory。irgan的reward也是放g上，也不直接生成，而且从候选里面用softmax挑

第二个问题: 目前对抗生成文本的方法都有哪些？说说你看过的相关论文吧

A:话说gumble softmax似乎还没人提过。
B:一种可生成离散数据的可导的方法，但是一直没弄得很明白过。
C:中午我刚看了gumble softmax，它可以代替policy gradient，直接可导了。
D:这是简单粗暴的解决办法。
E:但是看结果也并没有好到哪里去，算是比较有潜力的方法之一吧。
提供了一种思路。
F:毕竟只是近似。
G:目前的方法主要是这两种吗？policy gradient和gumble softmax，还有其他的吗？

有个基础的问题想请教一下，生成离散数据没法回传梯度，但隐层不是连续的么？为什么不直接用隐层的表示？像seq2seq 不也是在softmax 的输出基础上算的损失？

A:因为真实数据是离散的哦，为了给D和真实数据对应的生成数据，G所以也要生成离散的了。
B:因为在隐层上的细微更改没有意义



++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++++




